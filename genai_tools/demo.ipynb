{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich, json\n",
    "\n",
    "\n",
    "def print_json(data):\n",
    "    rich.print_json(json.dumps(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35.76\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta models found: ['meta.llama3-8b-instruct-v1:0', 'meta.llama3-70b-instruct-v1:0', 'meta.llama3-1-8b-instruct-v1:0:128k', 'meta.llama3-1-8b-instruct-v1:0', 'meta.llama3-1-70b-instruct-v1:0:128k', 'meta.llama3-1-70b-instruct-v1:0', 'meta.llama3-1-405b-instruct-v1:0', 'meta.llama3-2-11b-instruct-v1:0', 'meta.llama3-2-90b-instruct-v1:0', 'meta.llama3-2-1b-instruct-v1:0', 'meta.llama3-2-3b-instruct-v1:0']\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a Bedrock client\n",
    "bedrock = boto3.client(\"bedrock\")\n",
    "default_region = \"us-east-1\"\n",
    "\n",
    "# List all models from Amazon\n",
    "models = bedrock.list_foundation_models(\n",
    "    byProvider=\"Meta\",\n",
    ")\n",
    "\n",
    "nova_models = [\n",
    "    model[\"modelId\"]\n",
    "    for model in models[\"modelSummaries\"]\n",
    "    if \"meta\" in model[\"modelId\"].lower()\n",
    "]\n",
    "print(\"meta models found:\", nova_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \"Hello, World!\" program is a simple computer program that prints the text \"Hello, World!\" to the screen, serving as a traditional starting point for learning programming languages and testing the basic functionality of a programming environment.\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set the model ID.\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "# Set the prompt.\n",
    "prompt = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\", region_name=default_region)\n",
    "\n",
    "# Embed the prompt in Llama 3's instruction format.\n",
    "# More information: https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "formatted_prompt = f\"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "{prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\"\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_gen_len\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = bedrock_runtime.invoke_model(modelId=model_id, body=request)\n",
    "\n",
    "    # Decode the response body.\n",
    "    model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = model_response[\"generation\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_message_to_model(\n",
    "    conversation,\n",
    "    model_id=model_id,\n",
    "    max_tokens=512,\n",
    "    temperature=0.5,\n",
    "    top_p=0.9,\n",
    "    system_prompt=\"You are a helpful assistant focused in Meta Llama models\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Send a message to a model and return the response.\n",
    "\n",
    "    Args:\n",
    "        conversation (list): The conversation history/messages to send to the model.\n",
    "        model_id (str): The ID of the model to use.\n",
    "        max_tokens (int): Maximum number of tokens to generate in the response.\n",
    "        temperature (float): Sampling temperature to control randomness.\n",
    "        top_p (float): Nucleus sampling parameter to control the range of token sampling.\n",
    "        system_prompt (str): System prompt to guide the model's behavior.\n",
    "\n",
    "    Returns:\n",
    "        dict: The response from the model, containing the generated text and additional metadata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send the message to the model, using the provided inference configuration.\n",
    "        response = bedrock_runtime.converse(\n",
    "            modelId=model_id,\n",
    "            messages=conversation,\n",
    "            inferenceConfig={\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p,\n",
    "            },\n",
    "            system=[{\"text\": system_prompt}],\n",
    "        )\n",
    "\n",
    "        # Extract and print the response text.\n",
    "        print(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\n",
    "        return response\n",
    "\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Tesla Model X or Model S would be a good fit for a family of 4 humans and three cats. The Model X offers more cargo space and fold-flat second-row seats, making it easier to accommodate pet carriers or strollers. The Model S also has ample cargo space and a comfortable ride.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'd7062af4-83a4-4e0d-b5ce-ddaa0292397b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Mon, 09 Dec 2024 01:51:56 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '465',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'd7062af4-83a4-4e0d-b5ce-ddaa0292397b'},\n",
       "  'RetryAttempts': 0},\n",
       " 'output': {'message': {'role': 'assistant',\n",
       "   'content': [{'text': '\\n\\nThe Tesla Model X or Model S would be a good fit for a family of 4 humans and three cats. The Model X offers more cargo space and fold-flat second-row seats, making it easier to accommodate pet carriers or strollers. The Model S also has ample cargo space and a comfortable ride.'}]}},\n",
       " 'stopReason': 'end_turn',\n",
       " 'usage': {'inputTokens': 57, 'outputTokens': 63, 'totalTokens': 120},\n",
       " 'metrics': {'latencyMs': 709}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline parameters when invoking the model\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "system_prompt = \"You are a helpful assistant. Be concise and only respond with the answer to the question. Focus only in questions related to Tesla Cars.\"\n",
    "message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"What is a good fit for a family of 4 human and three cats?\"}],\n",
    "}\n",
    "conversation = []\n",
    "\n",
    "conversation.append(message)\n",
    "\n",
    "response = send_message_to_model(\n",
    "    conversation=conversation, model_id=model_id, system_prompt=system_prompt\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def stream_conversation(\n",
    "    bedrock_client,\n",
    "    model_id,\n",
    "    messages,\n",
    "    system_prompts,\n",
    "    inference_config,\n",
    "    additional_model_fields,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sends messages to a model and streams the response.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        messages (JSON) : The messages to send.\n",
    "        system_prompts (JSON) : The system prompts to send.\n",
    "        inference_config (JSON) : The inference configuration to use.\n",
    "        additional_model_fields (JSON) : Additional model fields to use.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Streaming messages with model %s\", model_id)\n",
    "\n",
    "    response = bedrock_client.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields,\n",
    "    )\n",
    "\n",
    "    stream = response.get(\"stream\")\n",
    "    if stream:\n",
    "        for event in stream:\n",
    "\n",
    "            if \"messageStart\" in event:\n",
    "                print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "            if \"contentBlockDelta\" in event:\n",
    "                print(event[\"contentBlockDelta\"][\"delta\"][\"text\"], end=\"\")\n",
    "\n",
    "            if \"messageStop\" in event:\n",
    "                print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "            if \"metadata\" in event:\n",
    "                metadata = event[\"metadata\"]\n",
    "                if \"usage\" in metadata:\n",
    "                    print(\"\\nToken usage\")\n",
    "                    print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                    print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                    print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                if \"metrics\" in event[\"metadata\"]:\n",
    "                    print(f\"Latency: {metadata['metrics']['latencyMs']} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Streaming messages with model meta.llama3-8b-instruct-v1:0\n",
      "ERROR:__main__:A client error occurred: You don't have access to the model with the specified model ID.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A client error occured: You don't have access to the model with the specified model ID.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "system_prompt = \"\"\"You are an app that creates playlists for a radio station\n",
    "  that plays rock and pop music. Only return song names and the artist.\"\"\"\n",
    "\n",
    "# Message to send to the model.\n",
    "input_text = \"Create a list of 3 pop songs.\"\n",
    "\n",
    "message = {\"role\": \"user\", \"content\": [{\"text\": input_text}]}\n",
    "conversation = [message]\n",
    "\n",
    "# System prompts.\n",
    "system_prompts = [{\"text\": system_prompt}]\n",
    "\n",
    "# inference parameters to use.\n",
    "temperature = 0.5\n",
    "\n",
    "# Base inference parameters.\n",
    "inference_config = {\"temperature\": temperature}\n",
    "\n",
    "# Additional model inference parameters.\n",
    "additional_model_fields = {}\n",
    "\n",
    "try:\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    stream_conversation(\n",
    "        bedrock_client,\n",
    "        model_id,\n",
    "        conversation,\n",
    "        system_prompts,\n",
    "        inference_config,\n",
    "        additional_model_fields,\n",
    "    )\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response[\"Error\"][\"Message\"]\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(\"A client error occured: \" + format(message))\n",
    "\n",
    "else:\n",
    "    print(f\"Finished streaming messages with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
